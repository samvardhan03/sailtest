# -*- coding: utf-8 -*-
"""Sailchat.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dvkvlOLTfNVfr-ZiDgK6c9cHi3rskq4l
"""

import json
import pickle
import random
import nltk
from nltk.stem.lancaster import LancasterStemmer
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder

# Download required NLTK data
nltk.download('punkt')

# Load intents data
with open('intents.json') as file:
    intents = json.load(file)

# Preprocess data
stemmer = LancasterStemmer()
words = []
labels = []
classes = set()  # Use a set to store unique intent tags

for intent in intents['intents']:
    tag = intent['tag']
    if isinstance(tag, list):
        tag = ' '.join(tag)  # Convert the list to a string
    classes.add(tag)
    for pattern in intent['patterns']:
        word = stemmer.stem(pattern.lower())  # Stemming for better accuracy
        words.append(word)
        labels.append(tag)

# Get unique intent tags
unique_labels = list(classes)

# Encode string labels to numeric values
label_encoder = LabelEncoder()
numeric_labels = label_encoder.fit_transform(unique_labels)

# Create a dictionary to map labels to numeric values
label_to_numeric = dict(zip(unique_labels, numeric_labels))

# Convert intent tags to numeric values
numeric_intent_tags = [label_to_numeric[tag] for tag in labels]

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(words, numeric_intent_tags, test_size=0.2, random_state=42)

# Tokenize and encode data (addressing potential padding errors)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
max_len = 50  # Adjust max_len based on your data and experiment
train_encodings = tokenizer(
    X_train, truncation=True, padding='max_length', return_tensors='tf', max_length=max_len
)
test_encodings = tokenizer(
    X_test, truncation=True, padding='max_length', return_tensors='tf', max_length=max_len
)

# Create TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((train_encodings, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((test_encodings, y_test))

# Fine-tune DistilBERT model (fixing optimizer error and addressing potential overfitting)
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(classes))

from tensorflow.keras.optimizers import RMSprop

# Create the optimizer object explicitly:
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)  # Use RMSprop optimizer

model.compile( optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)

# Train the model with validation split to prevent overfitting
history = model.fit(
    train_dataset.shuffle(1000).batch(16),
    epochs=100,
    validation_split=0.2,  # Use validation split for early stopping
    callbacks=[early_stop]
)

# Save the trained model and preprocessed data
model.save('model.h5')
pickle.dump({'tokenizer': tokenizer, 'classes': classes}, open('model_data.pkl', 'wb'))

print("Training complete! Model saved as 'model.h5' and preprocessed data saved as 'model_data.pkl'.")

